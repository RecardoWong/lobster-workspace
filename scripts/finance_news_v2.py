#!/usr/bin/env python3
"""
è´¢ç»æ–°é—»èšåˆå™¨ - å¤šæº + æ•°æ®ä¸­å¿ƒä¸“é¢˜
"""

import urllib.request
import json
import re
from datetime import datetime
import random

def fetch_with_timeout(url, headers, timeout=8):
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=timeout) as r:
            return r.read().decode('utf-8')
    except:
        return None

def fetch_sina():
    """æ–°æµªè´¢ç» - ç¾è‚¡/è´¢ç»"""
    try:
        url = 'https://feed.sina.com.cn/api/roll/get?pageid=153&lid=2516&num=10'
        data = fetch_with_timeout(url, {'User-Agent': 'Mozilla/5.0'})
        if data:
            parsed = json.loads(data)
            items = []
            if 'result' in parsed and 'data' in parsed['result']:
                for item in parsed['result']['data'][:4]:
                    items.append({
                        'title': item.get('title', ''),
                        'source': 'æ–°æµªè´¢ç»',
                        'url': item.get('url', ''),
                        'time': 'åˆšåˆš',
                        'tag': 'è´¢ç»',
                        'tagColor': '#ef4444'
                    })
            return items
    except:
        pass
    return []

def fetch_36kr():
    """36æ°ª - ç§‘æŠ€/AI/æ•°æ®ä¸­å¿ƒ"""
    try:
        url = 'https://36kr.com/api/newsflash/catalog'
        data = fetch_with_timeout(url, {'User-Agent': 'Mozilla/5.0'})
        if data:
            parsed = json.loads(data)
            items = []
            if parsed.get('data') and parsed['data'].get('newsflashList'):
                for item in parsed['data']['newsflashList'][:4]:
                    title = item.get('title', '')
                    # ä¼˜å…ˆæ•°æ®ä¸­å¿ƒå…³é”®è¯
                    dc_keywords = ['æ•°æ®ä¸­å¿ƒ', 'IDC', 'ç®—åŠ›', 'æœåŠ¡å™¨', 'AI', 'äººå·¥æ™ºèƒ½', 'äº‘è®¡ç®—', 'GPU']
                    is_dc = any(kw in title for kw in dc_keywords)
                    
                    items.append({
                        'title': title,
                        'source': '36æ°ª',
                        'url': f"https://36kr.com/newsflashes/{item.get('id', '')}",
                        'time': 'åˆšåˆš',
                        'tag': 'æ•°æ®ä¸­å¿ƒ' if is_dc else 'ç§‘æŠ€',
                        'tagColor': '#8b5cf6' if is_dc else '#f59e0b'
                    })
            return items
    except:
        pass
    return []

def fetch_wallstreet():
    """åå°”è¡—è§é—»"""
    try:
        url = 'https://api.wallstcn.com/apiv1/content/articles?page=1&limit=10'
        data = fetch_with_timeout(url, {'User-Agent': 'Mozilla/5.0'})
        if data:
            parsed = json.loads(data)
            items = []
            if parsed.get('data') and parsed['data'].get('items'):
                for item in parsed['data']['items'][:3]:
                    title = item.get('title', '')
                    dc_keywords = ['æ•°æ®', 'ç®—åŠ›', 'AI', 'æ•°æ®ä¸­å¿ƒ', 'IDC', 'äº‘è®¡ç®—', 'æœåŠ¡å™¨']
                    is_dc = any(kw in title for kw in dc_keywords)
                    
                    items.append({
                        'title': title,
                        'source': 'åå°”è¡—è§é—»',
                        'url': f"https://wallstreetcn.com/articles/{item.get('id', '')}",
                        'time': 'åˆšåˆš',
                        'tag': 'æ•°æ®ä¸­å¿ƒ' if is_dc else 'å…¨çƒ',
                        'tagColor': '#8b5cf6' if is_dc else '#10b981'
                    })
            return items
    except:
        pass
    return []

def aggregate():
    print(f'ğŸš€ æŠ“å–æ–°é—»... {datetime.now().strftime("%H:%M")}')
    
    # å¹¶è¡Œè·å–
    sina = fetch_sina()
    kr36 = fetch_36kr()
    wscn = fetch_wallstreet()
    
    # åˆå¹¶ - æ•°æ®ä¸­å¿ƒä¼˜å…ˆ
    all_news = kr36 + wscn + sina  # ç§‘æŠ€/æ•°æ®ä¸­å¿ƒæ–°é—»æ”¾å‰é¢
    
    # å»é‡
    seen = set()
    unique = []
    dc_count = 0
    for news in all_news:
        key = news['title'][:25]
        if key not in seen:
            seen.add(key)
            unique.append(news)
            if news['tag'] in ['æ•°æ®ä¸­å¿ƒ', 'ç®—åŠ›', 'IDC']:
                dc_count += 1
    
    # è¿‡æ»¤ï¼šåªä¿ç•™æœ‰çœŸå®URLé“¾æ¥çš„æ–°é—»ï¼ˆæ²¡æœ‰é“¾æ¥=å‡æ–°é—»ï¼‰
    valid_news = []
    for news in unique:
        url = news.get('url', '')
        # å¿…é¡»æ˜¯éç©ºçš„ã€ä¸æ˜¯#å ä½ç¬¦çš„ã€ä»¥httpå¼€å¤´çš„çœŸå®é“¾æ¥
        if url and url != '#' and url.startswith('http'):
            valid_news.append(news)
        else:
            print(f'âš ï¸ è¿‡æ»¤æ‰æ— æ¥æºçš„æ–°é—»: {news.get("title", "")[:30]}...')
    
    final_news = valid_news[:12]
    
    output = {
        'update_time': datetime.now().isoformat(),
        'source_count': 3,
        'total_count': len(final_news),
        'news': final_news
    }
    
    import os
    os.makedirs('/root/.openclaw/workspace/lobster-workspace/dashboard/data', exist_ok=True)
    
    with open('/root/.openclaw/workspace/lobster-workspace/dashboard/data/finance_news.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f'âœ… æ›´æ–° {len(final_news)} æ¡ (æ•°æ®ä¸­å¿ƒ: {dc_count} æ¡)')
    return final_news

if __name__ == '__main__':
    aggregate()
