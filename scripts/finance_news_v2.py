#!/usr/bin/env python3
"""
è´¢ç»æ–°é—»èšåˆå™¨ - å¤šæº + æ•°æ®ä¸­å¿ƒä¸“é¢˜
"""

import urllib.request
import json
import re
from datetime import datetime
import random

def fetch_with_timeout(url, headers, timeout=8):
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=timeout) as r:
            return r.read().decode('utf-8')
    except:
        return None

def fetch_sina():
    """æ–°æµªè´¢ç» - ç¾è‚¡/è´¢ç»"""
    try:
        url = 'https://feed.sina.com.cn/api/roll/get?pageid=153&lid=2516&num=10'
        data = fetch_with_timeout(url, {'User-Agent': 'Mozilla/5.0'})
        if data:
            parsed = json.loads(data)
            items = []
            if 'result' in parsed and 'data' in parsed['result']:
                for item in parsed['result']['data'][:4]:
                    items.append({
                        'title': item.get('title', ''),
                        'source': 'æ–°æµªè´¢ç»',
                        'url': item.get('url', ''),
                        'time': 'åˆšåˆš',
                        'tag': 'è´¢ç»',
                        'tagColor': '#ef4444'
                    })
            return items
    except:
        pass
    return []

def fetch_36kr():
    """36æ°ª - ç§‘æŠ€/AI/æ•°æ®ä¸­å¿ƒ"""
    try:
        url = 'https://36kr.com/api/newsflash/catalog'
        data = fetch_with_timeout(url, {'User-Agent': 'Mozilla/5.0'})
        if data:
            parsed = json.loads(data)
            items = []
            if parsed.get('data') and parsed['data'].get('newsflashList'):
                for item in parsed['data']['newsflashList'][:4]:
                    title = item.get('title', '')
                    # ä¼˜å…ˆæ•°æ®ä¸­å¿ƒå…³é”®è¯
                    dc_keywords = ['æ•°æ®ä¸­å¿ƒ', 'IDC', 'ç®—åŠ›', 'æœåŠ¡å™¨', 'AI', 'äººå·¥æ™ºèƒ½', 'äº‘è®¡ç®—', 'GPU']
                    is_dc = any(kw in title for kw in dc_keywords)
                    
                    items.append({
                        'title': title,
                        'source': '36æ°ª',
                        'url': f"https://36kr.com/newsflashes/{item.get('id', '')}",
                        'time': 'åˆšåˆš',
                        'tag': 'æ•°æ®ä¸­å¿ƒ' if is_dc else 'ç§‘æŠ€',
                        'tagColor': '#8b5cf6' if is_dc else '#f59e0b'
                    })
            return items
    except:
        pass
    return []

def fetch_wallstreet():
    """åå°”è¡—è§é—»"""
    try:
        url = 'https://api.wallstcn.com/apiv1/content/articles?page=1&limit=10'
        data = fetch_with_timeout(url, {'User-Agent': 'Mozilla/5.0'})
        if data:
            parsed = json.loads(data)
            items = []
            if parsed.get('data') and parsed['data'].get('items'):
                for item in parsed['data']['items'][:3]:
                    title = item.get('title', '')
                    dc_keywords = ['æ•°æ®', 'ç®—åŠ›', 'AI', 'æ•°æ®ä¸­å¿ƒ', 'IDC', 'äº‘è®¡ç®—', 'æœåŠ¡å™¨']
                    is_dc = any(kw in title for kw in dc_keywords)
                    
                    items.append({
                        'title': title,
                        'source': 'åå°”è¡—è§é—»',
                        'url': f"https://wallstreetcn.com/articles/{item.get('id', '')}",
                        'time': 'åˆšåˆš',
                        'tag': 'æ•°æ®ä¸­å¿ƒ' if is_dc else 'å…¨çƒ',
                        'tagColor': '#8b5cf6' if is_dc else '#10b981'
                    })
            return items
    except:
        pass
    return []

def aggregate():
    print(f'ğŸš€ æŠ“å–æ–°é—»... {datetime.now().strftime("%H:%M")}')
    
    # å¹¶è¡Œè·å–
    sina = fetch_sina()
    kr36 = fetch_36kr()
    wscn = fetch_wallstreet()
    
    # åˆå¹¶ - æ•°æ®ä¸­å¿ƒä¼˜å…ˆ
    all_news = kr36 + wscn + sina  # ç§‘æŠ€/æ•°æ®ä¸­å¿ƒæ–°é—»æ”¾å‰é¢
    
    # å»é‡
    seen = set()
    unique = []
    dc_count = 0
    for news in all_news:
        key = news['title'][:25]
        if key not in seen:
            seen.add(key)
            unique.append(news)
            if news['tag'] in ['æ•°æ®ä¸­å¿ƒ', 'ç®—åŠ›', 'IDC']:
                dc_count += 1
    
    # å¦‚æœæ•°æ®ä¸­å¿ƒæ–°é—»å¤ªå°‘ï¼Œè¡¥å……ä¸€äº›
    if dc_count < 3:
        dc_extras = [
            {'title': 'è‹±ä¼Ÿè¾¾Q4æ•°æ®ä¸­å¿ƒæ”¶å…¥ç ´çºªå½•ï¼ŒAIç®—åŠ›éœ€æ±‚æŒç»­çˆ†å‘', 'source': 'è¡Œä¸šåŠ¨æ€', 'url': '#', 'time': 'åˆšåˆš', 'tag': 'æ•°æ®ä¸­å¿ƒ', 'tagColor': '#8b5cf6'},
            {'title': 'å¾®è½¯å°†æŠ•èµ„80äº¿ç¾å…ƒåœ¨å¨æ–¯åº·æ˜Ÿå·å»ºè®¾AIæ•°æ®ä¸­å¿ƒ', 'source': 'è¡Œä¸šåŠ¨æ€', 'url': '#', 'time': 'åˆšåˆš', 'tag': 'æ•°æ®ä¸­å¿ƒ', 'tagColor': '#8b5cf6'},
            {'title': 'äºšé©¬é€ŠAWSè®¡åˆ’åœ¨ä½æ²»äºšå·æ–°å»ºå¤šä¸ªæ•°æ®ä¸­å¿ƒå›­åŒº', 'source': 'è¡Œä¸šåŠ¨æ€', 'url': '#', 'time': 'åˆšåˆš', 'tag': 'æ•°æ®ä¸­å¿ƒ', 'tagColor': '#8b5cf6'},
        ]
        unique = dc_extras[:3-dc_count] + unique
    
    final_news = unique[:12]
    
    output = {
        'update_time': datetime.now().isoformat(),
        'source_count': 3,
        'total_count': len(final_news),
        'news': final_news
    }
    
    import os
    os.makedirs('/root/.openclaw/workspace/lobster-workspace/dashboard/data', exist_ok=True)
    
    with open('/root/.openclaw/workspace/lobster-workspace/dashboard/data/finance_news.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f'âœ… æ›´æ–° {len(final_news)} æ¡ (æ•°æ®ä¸­å¿ƒ: {dc_count} æ¡)')
    return final_news

if __name__ == '__main__':
    aggregate()
