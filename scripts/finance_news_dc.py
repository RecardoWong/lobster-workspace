#!/usr/bin/env python3
"""
è´¢ç»æ–°é—»èšåˆå™¨ - åŒ…å«æ•°æ®ä¸­å¿ƒ/ç®—åŠ›/IDCä¸“é¢˜
"""

import urllib.request
import json
import re
from datetime import datetime

class NewsAggregator:
    def __init__(self):
        self.output_file = '/root/.openclaw/workspace/lobster-workspace/dashboard/data/finance_news.json'
        self.news_list = []
        
    def fetch_sina(self):
        """æ–°æµªè´¢ç»"""
        try:
            url = 'https://feed.sina.com.cn/api/roll/get?pageid=153&lid=2516&num=15'
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, timeout=10) as r:
                data = json.loads(r.read().decode())
                news_items = []
                if 'result' in data and 'data' in data['result']:
                    for item in data['result']['data'][:6]:
                        news_items.append({
                            'title': item.get('title', ''),
                            'source': 'æ–°æµªè´¢ç»',
                            'url': item.get('url', ''),
                            'time': 'åˆšåˆš',
                            'tag': 'è´¢ç»',
                            'tagColor': '#ef4444'
                        })
                print(f'âœ… æ–°æµªè´¢ç»: {len(news_items)} æ¡')
                return news_items
        except Exception as e:
            print(f'âŒ æ–°æµªè´¢ç»: {e}')
            return []
    
    def fetch_36kr(self):
        """36æ°ª - ç§‘æŠ€/æ•°æ®ä¸­å¿ƒ"""
        try:
            url = 'https://36kr.com/api/newsflash/catalog'
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, timeout=10) as r:
                data = json.loads(r.read().decode())
                news_items = []
                if data.get('data') and data['data'].get('newsflashList'):
                    for item in data['data']['newsflashList'][:4]:
                        title = item.get('title', '')
                        # ä¼˜å…ˆé€‰æ‹©æ•°æ®ä¸­å¿ƒç›¸å…³æ–°é—»
                        keywords = ['æ•°æ®ä¸­å¿ƒ', 'IDC', 'ç®—åŠ›', 'æœåŠ¡å™¨', 'AI', 'äººå·¥æ™ºèƒ½', 'äº‘è®¡ç®—', 'GPU']
                        is_dc_related = any(kw in title for kw in keywords)
                        
                        news_items.append({
                            'title': title,
                            'source': '36æ°ª',
                            'url': f"https://36kr.com/newsflashes/{item.get('id', '')}",
                            'time': 'åˆšåˆš',
                            'tag': 'æ•°æ®ä¸­å¿ƒ' if is_dc_related else 'ç§‘æŠ€',
                            'tagColor': '#8b5cf6' if is_dc_related else '#f59e0b'
                        })
                print(f'âœ… 36æ°ª: {len(news_items)} æ¡')
                return news_items
        except Exception as e:
            print(f'âŒ 36æ°ª: {e}')
            return []
    
    def fetch_wallstreet(self):
        """åå°”è¡—è§é—»"""
        try:
            url = 'https://api.wallstcn.com/apiv1/content/articles?page=1&limit=15'
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, timeout=10) as r:
                data = json.loads(r.read().decode())
                news_items = []
                if data.get('data') and data['data'].get('items'):
                    for item in data['data']['items'][:4]:
                        title = item.get('title', '')
                        # æ£€æŸ¥æ˜¯å¦æ•°æ®ä¸­å¿ƒç›¸å…³
                        keywords = ['æ•°æ®', 'ç®—åŠ›', 'AI', 'æ•°æ®ä¸­å¿ƒ', 'IDC', 'äº‘è®¡ç®—', 'æœåŠ¡å™¨']
                        is_dc_related = any(kw in title for kw in keywords)
                        
                        news_items.append({
                            'title': title,
                            'source': 'åå°”è¡—è§é—»',
                            'url': f"https://wallstreetcn.com/articles/{item.get('id', '')}",
                            'time': 'åˆšåˆš',
                            'tag': 'æ•°æ®ä¸­å¿ƒ' if is_dc_related else 'å…¨çƒ',
                            'tagColor': '#8b5cf6' if is_dc_related else '#10b981'
                        })
                print(f'âœ… åå°”è¡—è§é—»: {len(news_items)} æ¡')
                return news_items
        except Exception as e:
            print(f'âŒ åå°”è¡—è§é—»: {e}')
            return []
    
    def fetch_itnews(self):
        """ITä¹‹å®¶ - æ•°æ®ä¸­å¿ƒ/ç§‘æŠ€"""
        try:
            url = 'https://api.ithome.com/json/newslist/news?r=0'
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, timeout=10) as r:
                data = json.loads(r.read().decode())
                news_items = []
                if data.get('newslist'):
                    count = 0
                    for item in data['newslist']:
                        if count >= 4:
                            break
                        title = item.get('title', '')
                        # åªé€‰æ‹©æ•°æ®ä¸­å¿ƒ/ç§‘æŠ€ç›¸å…³
                        keywords = ['æ•°æ®', 'ç®—åŠ›', 'æœåŠ¡å™¨', 'IDC', 'AI', 'äº‘è®¡ç®—', 'GPU', 'èŠ¯ç‰‡']
                        if any(kw in title for kw in keywords):
                            news_items.append({
                                'title': title,
                                'source': 'ITä¹‹å®¶',
                                'url': item.get('url', ''),
                                'time': 'åˆšåˆš',
                                'tag': 'æ•°æ®ä¸­å¿ƒ',
                                'tagColor': '#8b5cf6'
                            })
                            count += 1
                print(f'âœ… ITä¹‹å®¶: {len(news_items)} æ¡')
                return news_items
        except Exception as e:
            print(f'âŒ ITä¹‹å®¶: {e}')
            return []
    
    def aggregate(self):
        print(f'\nğŸš€ æŠ“å–è´¢ç»æ–°é—»... {datetime.now().strftime("%Y-%m-%d %H:%M")}')
        print('=' * 60)
        
        sina_news = self.fetch_sina()
        kr36_news = self.fetch_36kr()
        wallstreet_news = self.fetch_wallstreet()
        itnews = self.fetch_itnews()
        
        # åˆå¹¶ï¼Œå»é‡
        all_news = itnews + kr36_news + wallstreet_news + sina_news
        
        seen = set()
        unique_news = []
        dc_count = 0
        
        for news in all_news:
            key = news['title'][:30]
            if key not in seen:
                seen.add(key)
                unique_news.append(news)
                if news['tag'] == 'æ•°æ®ä¸­å¿ƒ':
                    dc_count += 1
        
        self.news_list = unique_news[:15]  # æœ€å¤š15æ¡
        
        print(f'\nğŸ“Š æ€»è®¡: {len(self.news_list)} æ¡ (æ•°æ®ä¸­å¿ƒ: {dc_count} æ¡)')
        return self.news_list
    
    def save(self):
        output = {
            'update_time': datetime.now().isoformat(),
            'source_count': 4,
            'total_count': len(self.news_list),
            'news': self.news_list
        }
        
        import os
        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
        
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f'ğŸ’¾ å·²ä¿å­˜: {self.output_file}')
        return self.output_file

def main():
    aggregator = NewsAggregator()
    aggregator.aggregate()
    aggregator.save()
    print('\nâœ… å®Œæˆ!')

if __name__ == '__main__':
    main()
