#!/usr/bin/env python3
"""
Elon Musk æ¨ç‰¹ç›‘æ§ - ä¸“ä¸šåˆ†æç‰ˆ
åŸºäºMoltbook/ä¸“ä¸šAgentæ¨æ–‡åˆ†ææ–¹æ³•
"""

import os
import re
import json
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
from collections import Counter

class ElonMuskProAnalyzer:
    """é©¬æ–¯å…‹æ¨æ–‡ä¸“ä¸šåˆ†æå™¨"""
    
    def __init__(self):
        self.api_key = os.environ.get('TWITTERAPI_IO_KEY') or "new1_47751911508746daafaf9194b664aaed"
        self.base_url = "https://api.twitterapi.io/twitter"
        self.target_user = "elonmusk"
        self.history_file = "/tmp/elon_tweet_history.json"
        
        # æ”¹è¿›çš„å…³é”®è¯åº“
        self.keywords = {
            'crypto': {
                'terms': ['doge', 'dogecoin', 'bitcoin', 'btc', 'crypto', 'cryptocurrency', 
                         'blockchain', 'token', '$doge', '$btc', 'memecoin'],
                'impact_level': 'high',
                'typical_movement': 'Â±10-30%'
            },
            'tesla': {
                'terms': ['tesla', 'tsla', 'cybertruck', 'fsd', 'model s', 'model 3', 
                         'model x', 'model y', 'ev', 'electric vehicle'],
                'impact_level': 'medium',
                'typical_movement': 'Â±3-8%'
            },
            'spacex': {
                'terms': ['spacex', 'mars', 'rocket', 'starship', 'falcon', 'launch', 
                         'landing', 'space', 'starlink', 'satellite'],
                'impact_level': 'low',
                'typical_movement': 'æ¦‚å¿µç›¸å…³'
            },
            'ai_tech': {
                'terms': ['ai', 'artificial intelligence', 'neural', 'gpt', 'neuralink', 
                         'tech', 'technology', 'robot', 'optimus'],
                'impact_level': 'medium',
                'typical_movement': 'AIæ¦‚å¿µè‚¡'
            }
        }
        
        # è®½åˆº/ç©ç¬‘æ£€æµ‹
        self.sarcasm_markers = ['lol', 'haha', 'ğŸ˜‚', 'joke', 'jk', 'just kidding', 
                               'obviously', 'definitely', 'sure', 'totally', 'probably']
        
        # å¼ºåº¦è¯
        self.intensity_words = {
            'strong': ['massive', 'huge', 'incredible', 'amazing', 'revolutionary', 
                      'game changer', 'breakthrough', 'moon', 'mars'],
            'moderate': ['good', 'great', 'nice', 'cool', 'interesting'],
            'mild': ['ok', 'fine', 'maybe', 'perhaps']
        }
    
    def fetch_recent_tweets(self, hours: int = 25) -> List[Dict]:
        """è·å–æœ€è¿‘Nå°æ—¶çš„æ¨æ–‡"""
        # APIè°ƒç”¨é€»è¾‘...
        pass
    
    def analyze_tweet_pro(self, tweet: Dict) -> Dict:
        """ä¸“ä¸šçº§æ¨æ–‡åˆ†æï¼ˆ5å±‚åˆ†ææ³•ï¼‰"""
        
        text = tweet.get('text', '')
        text_lower = text.lower()
        created = tweet.get('createdAt', '')
        likes = tweet.get('likeCount', 0)
        retweets = tweet.get('retweetCount', 0)
        
        analysis = {
            'basic': {},
            'entities': {},
            'semantic': {},
            'impact': {},
            'recommendation': {}
        }
        
        # === ç¬¬1å±‚ï¼šåŸºç¡€ä¿¡æ¯ ===
        analysis['basic'] = {
            'id': tweet.get('id'),
            'created_at': created,
            'text': text,
            'likes': likes,
            'retweets': retweets,
            'replies': tweet.get('replyCount', 0),
            'engagement_rate': self._calc_engagement(likes, retweets, tweet.get('replyCount', 0)),
            'has_media': bool(tweet.get('media')),
            'is_reply': bool(tweet.get('inReplyToStatusId')),
            'is_retweet': bool(tweet.get('retweetedStatus'))
        }
        
        # === ç¬¬2å±‚ï¼šå®ä½“è¯†åˆ« ===
        mentions = re.findall(r'@(\w+)', text)
        cashtags = re.findall(r'\$([A-Za-z]+)', text)
        hashtags = re.findall(r'#(\w+)', text)
        urls = re.findall(r'https?://[^\s]+', text)
        
        # æ£€æµ‹ç›¸å…³é¢†åŸŸ
        detected_categories = []
        for category, data in self.keywords.items():
            if any(term in text_lower for term in data['terms']):
                detected_categories.append({
                    'category': category,
                    'impact': data['impact_level'],
                    'typical_move': data['typical_movement']
                })
        
        analysis['entities'] = {
            'mentions': mentions,
            'cashtags': cashtags,
            'hashtags': hashtags,
            'urls': urls,
            'categories': detected_categories
        }
        
        # === ç¬¬3å±‚ï¼šè¯­ä¹‰åˆ†æ ===
        # è®½åˆºæ£€æµ‹
        sarcasm_score = sum(1 for marker in self.sarcasm_markers if marker in text_lower)
        is_likely_sarcasm = sarcasm_score >= 1 and likes > 100000  # é«˜äº’åŠ¨+è®½åˆºæ ‡è®°
        
        # æƒ…æ„Ÿå¼ºåº¦
        intensity = 'neutral'
        for level, words in self.intensity_words.items():
            if any(w in text_lower for w in words):
                intensity = level
                break
        
        # æƒ…ç»ªææ€§
        sentiment = self._analyze_sentiment(text_lower)
        
        analysis['semantic'] = {
            'sentiment': sentiment,
            'intensity': intensity,
            'sarcasm_score': sarcasm_score,
            'is_likely_sarcasm': is_likely_sarcasm,
            'tone': 'playful' if is_likely_sarcasm else sentiment['type'],
            'key_phrases': self._extract_key_phrases(text)
        }
        
        # === ç¬¬4å±‚ï¼šå½±å“è¯„ä¼° ===
        # å†å²æ¨¡å¼åŒ¹é…
        historical_pattern = self._match_historical_pattern(text_lower, detected_categories)
        
        # æ—¶é—´æ•æ„Ÿæ€§
        time_context = self._analyze_time_context(created)
        
        # ç»¼åˆå½±å“è¯„åˆ†
        impact_score = self._calc_impact_score(
            likes, detected_categories, is_likely_sarcasm, 
            sentiment['score'], time_context
        )
        
        analysis['impact'] = {
            'score': impact_score,
            'level': 'high' if impact_score >= 8 else 'medium' if impact_score >= 5 else 'low',
            'historical_pattern': historical_pattern,
            'time_context': time_context,
            'predicted_assets': self._predict_affected_assets(detected_categories),
            'estimated_volatility': self._estimate_volatility(detected_categories, is_likely_sarcasm)
        }
        
        # === ç¬¬5å±‚ï¼šè¡ŒåŠ¨å»ºè®® ===
        analysis['recommendation'] = self._generate_recommendation(
            impact_score, detected_categories, is_likely_sarcasm, time_context
        )
        
        return analysis
    
    def _calc_engagement(self, likes: int, retweets: int, replies: int) -> float:
        """è®¡ç®—äº’åŠ¨ç‡åˆ†æ•°"""
        total = likes + retweets * 2 + replies * 3  # ä¸åŒæƒé‡
        if total > 500000:
            return 10.0
        elif total > 100000:
            return 7.0 + (total - 100000) / 400000 * 3
        elif total > 50000:
            return 5.0 + (total - 50000) / 50000 * 2
        else:
            return total / 50000 * 5
    
    def _analyze_sentiment(self, text: str) -> Dict:
        """åˆ†ææƒ…æ„Ÿ"""
        positive_words = ['love', 'great', 'amazing', 'awesome', 'bullish', 'moon', 'rocket']
        negative_words = ['hate', 'bad', 'terrible', 'bearish', 'crash', 'dump', 'scam']
        
        pos_count = sum(1 for w in positive_words if w in text)
        neg_count = sum(1 for w in negative_words if w in text)
        
        if pos_count > neg_count:
            return {'type': 'positive', 'score': min(pos_count * 2, 10)}
        elif neg_count > pos_count:
            return {'type': 'negative', 'score': min(neg_count * 2, 10)}
        else:
            return {'type': 'neutral', 'score': 5}
    
    def _extract_key_phrases(self, text: str) -> List[str]:
        """æå–å…³é”®çŸ­è¯­"""
        # ç®€åŒ–ç‰ˆï¼šæå–å¼•å·å†…å®¹å’Œé‡è¦å£°æ˜
        phrases = []
        
        # å¼•å·å†…å®¹
        quotes = re.findall(r'"([^"]+)"', text)
        phrases.extend(quotes)
        
        # å¤§å†™å¼ºè°ƒ
        caps = re.findall(r'\b[A-Z]{2,}\b', text)
        phrases.extend(caps[:3])  # æœ€å¤š3ä¸ª
        
        return phrases[:5]
    
    def _match_historical_pattern(self, text: str, categories: List[Dict]) -> str:
        """åŒ¹é…å†å²æ¨¡å¼"""
        if not categories:
            return 'general_comment'
        
        category = categories[0]['category']
        
        patterns = {
            'crypto': {
                'doge_direct': ['doge' in text, 'ç›´æ¥æåŠDOGEï¼Œé€šå¸¸å¼•å‘5-20%æ³¢åŠ¨'],
                'crypto_general': ['crypto' in text or 'bitcoin' in text, 'æ³›æ³›æåŠå¸åœˆï¼Œå½±å“è¾ƒå°'],
                'meme_coin': ['meme' in text, 'æåŠMemeæ¦‚å¿µï¼Œå¯èƒ½å¸¦åŠ¨ç›¸å…³å¸ç§']
            },
            'tesla': {
                'product_announce': ['cybertruck' in text or 'fsd' in text, 'äº§å“ç›¸å…³ï¼Œå…³æ³¨TSLA'],
                'production_update': ['production' in text or 'delivery' in text, 'ç”Ÿäº§æ•°æ®ï¼Œå½±å“è‚¡ä»·']
            }
        }
        
        cat_patterns = patterns.get(category, {})
        for pattern_name, (condition, description) in cat_patterns.items():
            if condition:
                return f"{pattern_name}: {description}"
        
        return 'no_specific_pattern'
    
    def _analyze_time_context(self, created: str) -> Dict:
        """åˆ†ææ—¶é—´ä¸Šä¸‹æ–‡"""
        try:
            # è§£ææ—¶é—´
            dt = datetime.fromisoformat(created.replace('Z', '+00:00'))
            hour = dt.hour
            weekday = dt.weekday()
            
            # åˆ¤æ–­å¸‚åœºæ—¶æ®µ
            context = {
                'hour': hour,
                'weekday': weekday,
                'is_trading_hours': 9 <= hour <= 16 and weekday < 5,  # ç¾è‚¡äº¤æ˜“æ—¶é—´
                'is_pre_market': 4 <= hour < 9 and weekday < 5,
                'is_after_hours': 16 <= hour <= 20 and weekday < 5,
                'is_weekend': weekday >= 5,
                'sensitivity': 'high' if (9 <= hour <= 16 and weekday < 5) else 'medium'
            }
            return context
        except:
            return {'sensitivity': 'unknown'}
    
    def _calc_impact_score(self, likes: int, categories: List[Dict], 
                          is_sarcasm: bool, sentiment_score: int, 
                          time_context: Dict) -> float:
        """è®¡ç®—ç»¼åˆå½±å“åˆ†æ•°"""
        score = 0
        
        # äº’åŠ¨åˆ†æ•° (0-3)
        if likes > 200000:
            score += 3
        elif likes > 100000:
            score += 2.5
        elif likes > 50000:
            score += 2
        elif likes > 10000:
            score += 1
        
        # ç±»åˆ«åˆ†æ•° (0-3)
        if categories:
            top_cat = categories[0]
            if top_cat['impact'] == 'high':
                score += 3
            elif top_cat['impact'] == 'medium':
                score += 2
            else:
                score += 1
        
        # æ—¶é—´æ•æ„Ÿæ€§ (0-2)
        if time_context.get('sensitivity') == 'high':
            score += 2
        else:
            score += 1
        
        # è®½åˆºæƒ©ç½š (å¦‚æœæ˜¯è®½åˆºï¼Œé™ä½å½±å“)
        if is_sarcasm:
            score *= 0.7
        
        # æƒ…æ„Ÿå¼ºåº¦åŠ æˆ
        if sentiment_score >= 8:
            score += 1
        
        return min(score, 10)
    
    def _predict_affected_assets(self, categories: List[Dict]) -> List[str]:
        """é¢„æµ‹å—å½±å“çš„èµ„äº§"""
        assets = []
        
        for cat in categories:
            cat_name = cat['category']
            if cat_name == 'crypto':
                assets.extend(['DOGE/USDT', 'DOGE/USD', 'BTC/USDT'])
            elif cat_name == 'tesla':
                assets.extend(['TSLA (ç¾è‚¡)', 'ç‰¹æ–¯æ‹‰æ¦‚å¿µè‚¡'])
            elif cat_name == 'spacex':
                assets.extend(['èˆªå¤©ETF (ITA)', 'SpaceXæœªä¸Šå¸‚'])
            elif cat_name == 'ai_tech':
                assets.extend(['AIæ¦‚å¿µè‚¡', 'NVDA', 'MSFT'])
        
        return list(set(assets))  # å»é‡
    
    def _estimate_volatility(self, categories: List[Dict], is_sarcasm: bool) -> str:
        """ä¼°è®¡æ³¢åŠ¨ç‡"""
        if not categories:
            return "é¢„è®¡æ— æ˜¾è‘—æ³¢åŠ¨"
        
        base_vol = categories[0].get('typical_move', 'Â±5%')
        
        if is_sarcasm:
            return f"{base_vol} (ä½†å¯èƒ½æ˜¯è®½åˆºï¼Œæ³¢åŠ¨å¯èƒ½çŸ­æš‚)"
        
        return base_vol
    
    def _generate_recommendation(self, impact_score: float, categories: List[Dict],
                                is_sarcasm: bool, time_context: Dict) -> Dict:
        """ç”Ÿæˆè¡ŒåŠ¨å»ºè®®"""
        
        rec = {
            'urgency': 'low',
            'action': 'observe',
            'timeline': 'ä¸‹æ¬¡æ£€æŸ¥',
            'details': [],
            'risks': []
        }
        
        if impact_score >= 8:
            rec['urgency'] = 'high'
            rec['action'] = 'immediate_attention'
            rec['timeline'] = 'ç«‹å³æŸ¥çœ‹ç›¸å…³èµ„äº§'
            rec['details'].append('é«˜å½±å“åŠ›æ¨æ–‡ï¼Œå¯èƒ½å¼•å‘å¸‚åœºå‰§çƒˆæ³¢åŠ¨')
            
        elif impact_score >= 5:
            rec['urgency'] = 'medium'
            rec['action'] = 'monitor_closely'
            rec['timeline'] = '30åˆ†é’Ÿå†…å…³æ³¨'
            rec['details'].append('ä¸­åº¦å½±å“ï¼Œå»ºè®®å…³æ³¨ç›¸å…³èµ„äº§ä»·æ ¼')
        
        else:
            rec['details'].append('å½±å“æœ‰é™ï¼Œå¸¸è§„è§‚å¯Ÿå³å¯')
        
        if is_sarcasm:
            rec['details'].append('âš ï¸ æ¨æ–‡å¯èƒ½ä¸ºè®½åˆº/ç©ç¬‘ï¼Œå¸‚åœºå¯èƒ½è¿‡åº¦ååº”')
            rec['risks'].append('è¿½æ¶¨æ€è·Œé£é™©')
        
        if time_context.get('is_trading_hours'):
            rec['details'].append('ç¾è‚¡äº¤æ˜“æ—¶é—´å†…ï¼Œè‚¡ç¥¨ç›¸å…³å½±å“æ›´ç›´æ¥')
        elif time_context.get('is_weekend'):
            rec['details'].append('å‘¨æœ«æ—¶æ®µï¼Œå¸åœˆååº”å¯èƒ½æ›´å¿«')
        
        return rec
    
    def generate_pro_report(self, analysis: Dict) -> str:
        """ç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Š"""
        lines = [
            "ğŸ¯ Elon Musk æ¨æ–‡ä¸“ä¸šåˆ†æ",
            f"â° åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            "=" * 70,
            ""
        ]
        
        # åŸºç¡€ä¿¡æ¯
        basic = analysis['basic']
        lines.append("ğŸ“‹ åŸºç¡€ä¿¡æ¯:")
        lines.append(f"  å‘å¸ƒæ—¶é—´: {basic['created_at']}")
        lines.append(f"  äº’åŠ¨æ•°æ®: â¤ï¸{basic['likes']:,} | ğŸ”„{basic['retweets']:,} | ğŸ’¬{basic['replies']:,}")
        lines.append(f"  äº’åŠ¨è¯„åˆ†: {basic['engagement_rate']:.1f}/10")
        lines.append("")
        
        # åŸæ–‡+ç¿»è¯‘
        lines.append("ğŸ“ æ¨æ–‡å†…å®¹:")
        lines.append(f"  {basic['text']}")
        
        translation = self._full_translate_v2(basic['text'])
        if translation:
            lines.append(f"\nğŸŒ ç¿»è¯‘:")
            lines.append(f"  {translation}")
        lines.append("")
        
        # è¯­ä¹‰åˆ†æ
        semantic = analysis['semantic']
        lines.append("ğŸ” è¯­ä¹‰åˆ†æ:")
        lines.append(f"  æƒ…ç»ª: {semantic['sentiment']['type']} (å¼ºåº¦: {semantic['sentiment']['score']}/10)")
        lines.append(f"  è¯­æ°”: {semantic['tone']}")
        lines.append(f"  è®½åˆºå¯èƒ½: {'æ˜¯ âš ï¸' if semantic['is_likely_sarcasm'] else 'å¦'}")
        if semantic['key_phrases']:
            lines.append(f"  å…³é”®çŸ­è¯­: {', '.join(semantic['key_phrases'])}")
        lines.append("")
        
        # å½±å“è¯„ä¼°
        impact = analysis['impact']
        lines.append("ğŸ’¹ å½±å“è¯„ä¼°:")
        lines.append(f"  å½±å“åˆ†æ•°: {impact['score']:.1f}/10 ({impact['level'].upper()})")
        lines.append(f"  é¢„è®¡æ³¢åŠ¨: {impact['estimated_volatility']}")
        
        if impact['predicted_assets']:
            lines.append(f"  ç›¸å…³èµ„äº§:")
            for asset in impact['predicted_assets'][:5]:
                lines.append(f"    â€¢ {asset}")
        
        if impact['historical_pattern'] != 'no_specific_pattern':
            lines.append(f"  å†å²æ¨¡å¼: {impact['historical_pattern']}")
        lines.append("")
        
        # è¡ŒåŠ¨å»ºè®®
        rec = analysis['recommendation']
        emoji = "ğŸ”´" if rec['urgency'] == 'high' else "ğŸŸ¡" if rec['urgency'] == 'medium' else "âšª"
        lines.append(f"{emoji} è¡ŒåŠ¨å»ºè®®:")
        lines.append(f"  ç´§æ€¥åº¦: {rec['urgency'].upper()}")
        lines.append(f"  å»ºè®®æ“ä½œ: {rec['action']}")
        lines.append(f"  æ—¶é—´çº¿: {rec['timeline']}")
        
        if rec['details']:
            lines.append(f"  è¯¦æƒ…:")
            for detail in rec['details']:
                lines.append(f"    â€¢ {detail}")
        
        if rec['risks']:
            lines.append(f"  âš ï¸ é£é™©æç¤º:")
            for risk in rec['risks']:
                lines.append(f"    â€¢ {risk}")
        
        lines.append("")
        lines.append("=" * 70)
        lines.append("ğŸ’¡ åŸºäºMoltbookä¸“ä¸šAgentåˆ†ææ–¹æ³•ç”Ÿæˆ")
        
        return "\n".join(lines)
    
    def _full_translate_v2(self, text: str) -> str:
        """æ”¹è¿›çš„ç¿»è¯‘"""
        translations = {
            'dogecoin': 'ç‹—ç‹—å¸', 'doge': 'DOGE',
            'bitcoin': 'æ¯”ç‰¹å¸', 'btc': 'BTC',
            'to the moon': 'å»æœˆçƒï¼ˆæš´æ¶¨ï¼‰',
            'rocket': 'ğŸš€ç«ç®­',
            'tesla': 'ç‰¹æ–¯æ‹‰', 'tsla': 'TSLAè‚¡ç¥¨',
            'cybertruck': 'èµ›åšçš®å¡',
            'mars': 'ç«æ˜Ÿ',
        }
        
        result = text
        for eng, chn in translations.items():
            result = re.sub(r'\b' + re.escape(eng) + r'\b', chn, result, flags=re.IGNORECASE)
        
        return result if result != text else ""


def main():
    """æµ‹è¯•ä¸“ä¸šåˆ†æ"""
    analyzer = ElonMuskProAnalyzer()
    
    # æ¨¡æ‹Ÿæ¨æ–‡æµ‹è¯•
    test_tweets = [
        {
            'id': 'test1',
            'text': 'Dogecoin to the moon ğŸš€',
            'createdAt': '2026-02-11T10:00:00Z',
            'likeCount': 150000,
            'retweetCount': 45000,
            'replyCount': 8000
        },
        {
            'id': 'test2',
            'text': 'Tesla FSD is getting better every day lol',
            'createdAt': '2026-02-11T09:00:00Z',
            'likeCount': 80000,
            'retweetCount': 12000,
            'replyCount': 3000
        }
    ]
    
    for tweet in test_tweets:
        analysis = analyzer.analyze_tweet_pro(tweet)
        report = analyzer.generate_pro_report(analysis)
        print(report)
        print("\n" + "="*70 + "\n")


if __name__ == "__main__":
    main()
