#!/usr/bin/env python3
"""
Elon Musk æ¨ç‰¹ç›‘æ§ - ä¸“ä¸š5å±‚åˆ†æç‰ˆ
åŸºäºMoltbook/ä¸“ä¸šAgentæ¨æ–‡åˆ†ææ–¹æ³•
"""

import os
import re
import json
import urllib.request
import urllib.parse
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
from collections import Counter

class ElonMuskMonitor:
    """é©¬æ–¯å…‹æ¨ç‰¹ä¸“ä¸šç›‘æ§å™¨"""
    
    def __init__(self):
        self.api_key = os.environ.get('TWITTERAPI_IO_KEY') or "new1_47751911508746daafaf9194b664aaed"
        self.base_url = "https://api.twitterapi.io/twitter"
        self.target_user = "elonmusk"
        self.history_file = "/tmp/elon_tweet_history.json"
        
        # ä¸“ä¸šå…³é”®è¯åº“
        self.keywords = {
            'crypto': {
                'terms': ['doge', 'dogecoin', 'bitcoin', 'btc', 'crypto', 'cryptocurrency', 
                         'blockchain', 'token', '$doge', '$btc', 'memecoin'],
                'impact_level': 'high',
                'typical_movement': 'Â±10-30%'
            },
            'tesla': {
                'terms': ['tesla', 'tsla', 'cybertruck', 'fsd', 'model s', 'model 3', 
                         'model x', 'model y', 'ev', 'electric vehicle'],
                'impact_level': 'medium',
                'typical_movement': 'Â±3-8%'
            },
            'spacex': {
                'terms': ['spacex', 'mars', 'rocket', 'starship', 'falcon', 'launch', 
                         'landing', 'space', 'starlink', 'satellite'],
                'impact_level': 'low',
                'typical_movement': 'æ¦‚å¿µç›¸å…³'
            },
            'ai_tech': {
                'terms': ['ai', 'artificial intelligence', 'neural', 'gpt', 'neuralink', 
                         'tech', 'technology', 'robot', 'optimus'],
                'impact_level': 'medium',
                'typical_movement': 'AIæ¦‚å¿µè‚¡'
            }
        }
        
        # è®½åˆº/ç©ç¬‘æ£€æµ‹
        self.sarcasm_markers = ['lol', 'haha', 'ğŸ˜‚', 'joke', 'jk', 'just kidding', 
                               'obviously', 'definitely', 'sure', 'totally', 'probably']
        
        # å¼ºåº¦è¯
        self.intensity_words = {
            'strong': ['massive', 'huge', 'incredible', 'amazing', 'revolutionary', 
                      'game changer', 'breakthrough', 'moon', 'mars'],
            'moderate': ['good', 'great', 'nice', 'cool', 'interesting'],
            'mild': ['ok', 'fine', 'maybe', 'perhaps']
        }
    
    def _make_request(self, endpoint: str, params: Dict = None) -> Dict:
        """å‘é€APIè¯·æ±‚"""
        url = f"{self.base_url}{endpoint}"
        if params:
            query = '&'.join([f"{k}={urllib.parse.quote(str(v))}" for k, v in params.items()])
            url = f"{url}?{query}"
        
        headers = {
            'X-API-Key': self.api_key,
            'User-Agent': 'ElonMonitor/1.0'
        }
        
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=30) as response:
                return json.loads(response.read().decode())
        except Exception as e:
            return {'error': str(e)}
    
    def get_latest_tweets(self, max_results: int = 10) -> List[Dict]:
        """è·å–æœ€æ–°æ¨æ–‡"""
        endpoint = "/tweet/advanced_search"
        params = {
            'query': f'from:{self.target_user}',
            'queryType': 'Latest',
            'count': max_results
        }
        
        result = self._make_request(endpoint, params)
        return result.get('tweets', [])
    
    def check_new_tweets(self) -> Tuple[bool, List[Dict]]:
        """æ£€æŸ¥æ–°æ¨æ–‡"""
        tweets = self.get_latest_tweets(max_results=10)
        
        # è¯»å–å†å²
        history = []
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r') as f:
                    history = json.load(f)
            except:
                pass
        
        # æ‰¾æ–°æ¨æ–‡
        history_ids = {h.get('id') for h in history}
        new_tweets = [t for t in tweets if t.get('id') not in history_ids]
        
        # ä¿å­˜å½“å‰è®°å½•
        if tweets:
            with open(self.history_file, 'w') as f:
                json.dump(tweets[:20], f)
        
        return len(new_tweets) > 0, new_tweets
    
    def analyze_tweet_pro(self, tweet: Dict) -> Dict:
        """ä¸“ä¸š5å±‚åˆ†æ"""
        
        # ä¼˜å…ˆè·å–å®Œæ•´æ–‡æœ¬ï¼ˆTwitter APIå¯èƒ½æœ‰full_textå­—æ®µï¼‰
        text = tweet.get('full_text', '') or tweet.get('text', '')
        text_lower = text.lower()
        created = tweet.get('createdAt', '')
        likes = tweet.get('likeCount', 0)
        retweets = tweet.get('retweetCount', 0)
        replies = tweet.get('replyCount', 0)
        
        analysis = {}
        
        # === Layer 1: åŸºç¡€ä¿¡æ¯ ===
        analysis['basic'] = {
            'id': tweet.get('id'),
            'created_at': created,
            'text': text,  # ä¼˜å…ˆä½¿ç”¨å¯èƒ½å­˜åœ¨çš„å®Œæ•´æ–‡æœ¬
            'full_text': tweet.get('full_text', text),  # ä¿å­˜å®Œæ•´æ–‡æœ¬å¤‡ç”¨
            'likes': likes,
            'retweets': retweets,
            'replies': replies,
            'engagement_score': min((likes + retweets * 2 + replies * 3) / 50000, 10)
        }
        
        # === Layer 2: å®ä½“è¯†åˆ« ===
        mentions = re.findall(r'@(\w+)', text)
        cashtags = re.findall(r'\$([A-Za-z]+)', text)
        hashtags = re.findall(r'#(\w+)', text)
        
        detected_categories = []
        for category, data in self.keywords.items():
            if any(term in text_lower for term in data['terms']):
                detected_categories.append({
                    'category': category,
                    'impact': data['impact_level'],
                    'typical_move': data['typical_movement']
                })
        
        analysis['entities'] = {
            'mentions': mentions,
            'cashtags': cashtags,
            'hashtags': hashtags,
            'categories': detected_categories
        }
        
        # === Layer 3: è¯­ä¹‰åˆ†æ ===
        sarcasm_score = sum(1 for marker in self.sarcasm_markers if marker in text_lower)
        is_sarcasm = sarcasm_score >= 1 and likes > 100000
        
        intensity = 'neutral'
        for level, words in self.intensity_words.items():
            if any(w in text_lower for w in words):
                intensity = level
                break
        
        # æƒ…ç»ªåˆ†æ
        positive = ['love', 'great', 'amazing', 'awesome', 'bullish', 'moon', 'rocket']
        negative = ['hate', 'bad', 'terrible', 'bearish', 'crash', 'dump']
        pos_count = sum(1 for w in positive if w in text_lower)
        neg_count = sum(1 for w in negative if w in text_lower)
        sentiment = 'positive' if pos_count > neg_count else 'negative' if neg_count > pos_count else 'neutral'
        
        analysis['semantic'] = {
            'sentiment': sentiment,
            'intensity': intensity,
            'is_sarcasm': is_sarcasm,
            'sarcasm_warning': 'âš ï¸ å¯èƒ½ä¸ºè®½åˆº/ç©ç¬‘' if is_sarcasm else 'å¦'
        }
        
        # === Layer 4: å½±å“è¯„ä¼° ===
        # è®¡ç®—å½±å“åˆ†æ•°
        score = 0
        if likes > 200000: score += 3
        elif likes > 100000: score += 2.5
        elif likes > 50000: score += 2
        elif likes > 10000: score += 1
        
        if detected_categories:
            if detected_categories[0]['impact'] == 'high': score += 3
            elif detected_categories[0]['impact'] == 'medium': score += 2
            else: score += 1
        
        if is_sarcasm: score *= 0.7
        
        impact_level = 'high' if score >= 8 else 'medium' if score >= 5 else 'low'
        
        # é¢„æµ‹å—å½±å“èµ„äº§
        assets = []
        for cat in detected_categories:
            if cat['category'] == 'crypto':
                assets.extend(['DOGE/USDT', 'BTC/USDT', 'DOGE/USD'])
            elif cat['category'] == 'tesla':
                assets.extend(['TSLA (ç¾è‚¡)', 'ç‰¹æ–¯æ‹‰æ¦‚å¿µè‚¡'])
            elif cat['category'] == 'spacex':
                assets.extend(['èˆªå¤©ETF', 'SpaceXç›¸å…³'])
        
        analysis['impact'] = {
            'score': round(score, 1),
            'level': impact_level,
            'level_emoji': 'ğŸ”´' if impact_level == 'high' else 'ğŸŸ¡' if impact_level == 'medium' else 'âšª',
            'predicted_assets': list(set(assets))[:5],
            'volatility_estimate': detected_categories[0]['typical_move'] if detected_categories else 'Â±5%'
        }
        
        # === Layer 5: è¡ŒåŠ¨å»ºè®® ===
        if impact_level == 'high':
            action = {'urgency': 'HIGH', 'action': 'ç«‹å³å…³æ³¨', 'timeline': 'é©¬ä¸ŠæŸ¥çœ‹'}
        elif impact_level == 'medium':
            action = {'urgency': 'MEDIUM', 'action': 'å¯†åˆ‡ç›‘æ§', 'timeline': '30åˆ†é’Ÿå†…'}
        else:
            action = {'urgency': 'LOW', 'action': 'å¸¸è§„è§‚å¯Ÿ', 'timeline': 'ä¸‹æ¬¡æ£€æŸ¥'}
        
        if is_sarcasm:
            action['warning'] = 'æ¨æ–‡å¯èƒ½ä¸ºè®½åˆºï¼Œå¸‚åœºå¯èƒ½è¿‡åº¦ååº”ï¼Œè°¨æ…è¿½æ¶¨'
        
        analysis['recommendation'] = action
        
        return analysis
    
    def generate_pro_alert(self, analyses: List[Dict]) -> str:
        """ç”Ÿæˆä¸“ä¸šæ¨é€"""
        lines = [
            "ğŸš¨ ELON MUSK æ–°æ¨æ–‡æ£€æµ‹",
            f"â° {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            "=" * 70,
            ""
        ]
        
        for i, analysis in enumerate(analyses, 1):
            basic = analysis['basic']
            entities = analysis['entities']
            semantic = analysis['semantic']
            impact = analysis['impact']
            rec = analysis['recommendation']
            
            # æ ‡é¢˜
            lines.append(f"\n{impact['level_emoji']} æ¨æ–‡ #{i} | å½±å“çº§åˆ«: {impact['level']}")
            lines.append(f"â° å‘å¸ƒæ—¶é—´: {basic['created_at'][:16]}")
            lines.append("-" * 70)
            
            # Layer 1: åŸºç¡€
            lines.append("\nğŸ“‹ åŸºç¡€ä¿¡æ¯:")
            lines.append(f"  äº’åŠ¨: â¤ï¸{basic['likes']:,} | ğŸ”„{basic['retweets']:,} | ğŸ’¬{basic['replies']:,}")
            lines.append(f"  äº’åŠ¨è¯„åˆ†: {basic['engagement_score']:.1f}/10")
            
            # åŸæ–‡+ç¿»è¯‘ - åŒæ—¶æ˜¾ç¤ºè‹±æ–‡å’Œä¸­æ–‡
            full_text = basic.get('full_text', basic['text'])
            tweet_id = basic.get('id', '')
            
            lines.append(f"\nğŸ“ è‹±æ–‡åŸæ–‡:")
            lines.append(f"  {full_text}")
            
            # è‡ªåŠ¨ç”Ÿæˆä¸­æ–‡ç¿»è¯‘
            lines.append(f"\nğŸŒ ä¸­æ–‡ç¿»è¯‘:")
            chinese_translation = self._translate_to_chinese(full_text)
            lines.append(f"  {chinese_translation}")
            
            # å¦‚æœè¢«æˆªæ–­ï¼Œæä¾›é“¾æ¥
            if 'â€¦' in full_text or full_text.endswith('...') or len(full_text) > 280:
                if tweet_id:
                    lines.append(f"\n  ğŸ”— æŸ¥çœ‹å®Œæ•´æ¨æ–‡: https://x.com/elonmusk/status/{tweet_id}")
            
            # Layer 2: å®ä½“
            if entities['categories']:
                lines.append(f"\nğŸ·ï¸ ç›¸å…³é¢†åŸŸ:")
                for cat in entities['categories']:
                    lines.append(f"  â€¢ {cat['category']} ({cat['impact']})")
            
            if entities['mentions']:
                lines.append(f"  æåŠ: {', '.join(entities['mentions'][:3])}")
            
            # Layer 3: è¯­ä¹‰
            lines.append(f"\nğŸ” è¯­ä¹‰åˆ†æ:")
            lines.append(f"  æƒ…ç»ª: {semantic['sentiment']} | å¼ºåº¦: {semantic['intensity']}")
            if semantic['is_sarcasm']:
                lines.append(f"  âš ï¸ è®½åˆºè­¦å‘Š: {semantic['sarcasm_warning']}")
            
            # Layer 4: å½±å“
            lines.append(f"\nğŸ’¹ å½±å“è¯„ä¼°:")
            lines.append(f"  å½±å“åˆ†æ•°: {impact['score']}/10")
            lines.append(f"  é¢„è®¡æ³¢åŠ¨: {impact['volatility_estimate']}")
            if impact['predicted_assets']:
                lines.append(f"  ç›¸å…³èµ„äº§:")
                for asset in impact['predicted_assets']:
                    lines.append(f"    â€¢ {asset}")
            
            # Layer 5: å»ºè®®
            lines.append(f"\n{impact['level_emoji']} è¡ŒåŠ¨å»ºè®®:")
            lines.append(f"  ç´§æ€¥åº¦: {rec['urgency']}")
            lines.append(f"  æ“ä½œå»ºè®®: {rec['action']}")
            lines.append(f"  æ—¶é—´çº¿: {rec['timeline']}")
            if 'warning' in rec:
                lines.append(f"  âš ï¸ æé†’: {rec['warning']}")
            
            lines.append("\n" + "=" * 70)
        
        lines.append("\nğŸ’¡ åŸºäºMoltbookä¸“ä¸šAgent 5å±‚åˆ†ææ³•")
        return "\n".join(lines)
    
    def _translate(self, text: str) -> str:
        """ç¿»è¯‘å…³é”®æœ¯è¯­"""
        translations = {
            'dogecoin': 'ç‹—ç‹—å¸', 'doge': 'DOGE',
            'bitcoin': 'æ¯”ç‰¹å¸', 'btc': 'BTC',
            'to the moon': 'å»æœˆçƒï¼ˆæš´æ¶¨ï¼‰',
            'rocket': 'ğŸš€ç«ç®­',
            'tesla': 'ç‰¹æ–¯æ‹‰', 'tsla': 'TSLA',
            'cybertruck': 'èµ›åšçš®å¡',
            'mars': 'ç«æ˜Ÿ',
            'launch': 'å‘å°„',
            'ai': 'AI',
            'artificial intelligence': 'äººå·¥æ™ºèƒ½'
        }
        
        result = text
        for eng, chn in translations.items():
            result = re.sub(r'\b' + re.escape(eng) + r'\b', chn, result, flags=re.IGNORECASE)
        
        return result if result != text else ""
    
    def _translate_to_chinese(self, text: str) -> str:
        """æ•´å¥ç¿»è¯‘ä¸ºä¸­æ–‡"""
        import re
        
        # å¦‚æœæ˜¯çº¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            return text
        
        # å¸¸ç”¨çŸ­è¯­æ•´å¥ç¿»è¯‘
        translations = {
            # é—®å€™/æ—¥å¸¸
            r'\bgm\b': 'æ—©ä¸Šå¥½',
            r'\bgn\b': 'æ™šå®‰', 
            r'\bhello\b': 'ä½ å¥½',
            r'\bhi\b': 'å—¨',
            r'\bthanks?\b': 'è°¢è°¢',
            r'\bthank you\b': 'è°¢è°¢ä½ ',
            
            # æƒ…ç»ªè¡¨è¾¾
            r'\blol\b': 'å“ˆå“ˆ',
            r'\bhaha\b': 'å“ˆå“ˆ',
            r'\bwow\b': 'å“‡',
            r'\bamazing\b': 'å¤ªæ£’äº†',
            r'\bawesome\b': 'å‰å®³',
            r'\bterrible\b': 'ç³Ÿç³•',
            r'\bgreat\b': 'å¾ˆæ£’',
            r'\bgood\b': 'å¥½çš„',
            r'\bbravo\b': 'å¤ªæ£’äº†',
            r'\babsolutely\b': 'å®Œå…¨åŒæ„',
            r'\btrue\b': 'ç¡®å®',
            r'\bugh\b': 'å‘ƒ/å”‰',
            r'\bdisturbing\b': 'ä»¤äººä¸å®‰',
            
            # å¸åœˆ/è‚¡ç¥¨
            r'\bdogecoin\b': 'ç‹—ç‹—å¸',
            r'\bdoge\b': 'ç‹—ç‹—å¸',
            r'\bbitcoin\b': 'æ¯”ç‰¹å¸',
            r'\bbtc\b': 'æ¯”ç‰¹å¸',
            r'\bto the moon\b': 'æš´æ¶¨/å»æœˆçƒ',
            r'\bro\b': 'ç«ç®­',
            r'\bcrypto\b': 'åŠ å¯†è´§å¸',
            r'\btesla\b': 'ç‰¹æ–¯æ‹‰',
            r'\btsla\b': 'ç‰¹æ–¯æ‹‰è‚¡ç¥¨',
            r'\bstock\b': 'è‚¡ç¥¨',
            r'\bmarket\b': 'å¸‚åœº',
            
            # SpaceX
            r'\bspacex\b': 'SpaceX',
            r'\bmars\b': 'ç«æ˜Ÿ',
            r'\brocket\b': 'ç«ç®­',
            r'\blaunch\b': 'å‘å°„',
            
            # å¸¸ç”¨å¥å‹
            r'\bi think\b': 'æˆ‘è®¤ä¸º',
            r'\bi believe\b': 'æˆ‘ç›¸ä¿¡',
        }
        
        translated = text
        for pattern, chinese in translations.items():
            translated = re.sub(pattern, chinese, translated, flags=re.IGNORECASE)
        
        # å¦‚æœç¿»è¯‘åæœ‰æ˜æ˜¾å˜åŒ–ï¼Œè¿”å›ç¿»è¯‘
        if translated != text:
            return translated
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å¤æ‚å¥å‹ï¼Œè¿”å›åŸæ–‡+æç¤º
        return text + "\n  [è‹±æ–‡æ¨æ–‡ï¼Œå¯ç‚¹å‡»é“¾æ¥æŸ¥çœ‹åŸæ–‡]"


def main():
    """ä¸»å‡½æ•°"""
    monitor = ElonMuskMonitor()
    
    # æ£€æŸ¥æ–°æ¨æ–‡
    has_new, new_tweets = monitor.check_new_tweets()
    
    if has_new:
        print(f"ğŸ”” å‘ç° {len(new_tweets)} æ¡æ–°æ¨æ–‡ï¼\n")
        
        # ä¸“ä¸š5å±‚åˆ†ææ¯æ¡æ¨æ–‡
        analyses = [monitor.analyze_tweet_pro(t) for t in new_tweets]
        
        # ç”Ÿæˆä¸“ä¸šæ¨é€
        alert = monitor.generate_pro_alert(analyses)
        print(alert)
        
        # ä¿å­˜
        filename = f"/tmp/elon_pro_{datetime.now().strftime('%Y%m%d_%H%M')}.txt"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(alert)
        print(f"\nğŸ’¾ ä¸“ä¸šåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {filename}")
        
    else:
        print("ğŸ“­ é©¬æ–¯å…‹æš‚æ— æ–°æ¨æ–‡")


if __name__ == "__main__":
    main()
